baseUrl: "https://api.targon.com/v1"
endpoints:
  - path: /completions
    externalPath: /completions
    method: POST
    auth:
      type: header
      key: "Authorization"
      value: "Bearer {{api-key}}"
    headers:
      Content-Type: application/json
    requestSchema:
      type: object
      properties:
        model:
          type: string
          default: "deepseek-ai/DeepSeek-R1"
          enum: [
            "deepseek-ai/DeepSeek-R1",
            "deepseek-ai/DeepSeek-V3"
          ]
          description: "The model to use for completion."
        prompt:
          type: string
          description: "Text to generate completion for."
        temperature:
          type: number
          default: 0.7
          description: "Controls randomness in the response. Lower is more deterministic."
        max_tokens:
          type: integer
          default: 256
          description: "Maximum number of tokens to generate."
        top_p:
          type: number
          default: 0.1
          description: "Controls diversity via nucleus sampling."
        frequency_penalty:
          type: number
          default: 0
          description: "Reduces repetition of token sequences."
        presence_penalty:
          type: number
          default: 0
          description: "Reduces repetition of topics."
        stream:
          type: boolean
          default: true
          description: "Whether to stream the response."
        logprobs:
          type: boolean
          default: false
          description: "Whether to return log probabilities of the output tokens."
      required: [ model, prompt ]

  - path: /chat
    externalPath: /chat/completions
    method: POST
    auth:
      type: header
      key: "Authorization"
      value: "Bearer {{api-key}}"
    headers:
      Content-Type: application/json
    requestSchema:
      type: object
      properties:
        model:
          type: string
          default: "deepseek-ai/DeepSeek-R1"
          enum: [
            "deepseek-ai/DeepSeek-R1",
            "deepseek-ai/DeepSeek-V3"
          ]
          description: "The model to use for chat completion."
        messages:
          type: array
          items:
            type: object
            properties:
              role:
                type: string
                enum: [ user, assistant, system ]
              content:
                type: string
            required: [ role, content ]
        temperature:
          type: number
          default: 0.7
          description: "Controls randomness in the response. Lower is more deterministic."
        max_tokens:
          type: integer
          default: 256
          description: "Maximum number of tokens to generate."
        top_p:
          type: number
          default: 0.1
          description: "Controls diversity via nucleus sampling."
        frequency_penalty:
          type: number
          default: 0
          description: "Reduces repetition of token sequences."
        presence_penalty:
          type: number
          default: 0
          description: "Reduces repetition of topics."
        stream:
          type: boolean
          default: true
          description: "Whether to stream the response."
        logprobs:
          type: boolean
          default: false
          description: "Whether to return log probabilities of the output tokens."
      required: [ model, messages ]
