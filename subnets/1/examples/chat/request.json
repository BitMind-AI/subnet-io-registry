{
  "model": "llama-3",
  "messages": [
    {
      "role": "user",
      "content": "Hello, how are you?"
    }
  ],
  "temperature": 0.1,
  "max_tokens": 500,
  "top_p": 1,
  "stream": false,
  "logprobs": false
}
