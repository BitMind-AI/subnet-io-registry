baseUrl: "https://llm.chutes.ai/v1"
name: "Serverless Computing (Subnet 64)"
description: "Run high-performance models like DeepSeek on serverless infrastructure."
endpoints:
  - path: /chat/completions
    externalPath: /chat/completions
    method: POST
    auth:
      type: header
      key: "Authorization"
      value: "Bearer {{api-key}}"
    headers:
      Content-Type: application/json
    requestSchema:
      type: object
      properties:
        model:
          type: string
          default: "deepseek-ai/DeepSeek-V3-0324"
          enum:
            [
              "deepseek-ai/DeepSeek-V3-0324",
              "chutesai/Mistral-Small-3.1-24B-Instruct-2503",
              "deepseek-ai/DeepSeek-R1",
              "chutesai/Llama-4-Maverick-17B-128E-Instruct-FP8",
              "deepseek-ai/DeepSeek-V3",
              "Qwen/Qwen2.5-VL-32B-Instruct",
              "deepseek-ai/DeepSeek-R1-Zero",
              "nvidia/Llama-3_1-Nemotron-Ultra-253B-v1",
              "chutesai/Llama-4-Scout-17B-16E-Instruct",
              "deepseek-ai/DeepSeek-V3-Base",
              "moonshotai/Kimi-VL-A3B-Thinking",
              "cognitivecomputations/Dolphin3.0-Mistral-24B",
              "cognitivecomputations/Dolphin3.0-R1-Mistral-24B",
              "microsoft/MAI-DS-R1-FP8",
              "THUDM/GLM-4-32B-0414",
              "agentica-org/DeepCoder-14B-Preview",
              "shisa-ai/shisa-v2-llama3.3-70b",
              "THUDM/GLM-Z1-32B-0414",
              "ArliAI/QwQ-32B-ArliAI-RpR-v1",
              "chutesai/Llama-3.1-405B-FP8",
            ]
          description: "The model to use for chat completion."
        messages:
          type: array
          items:
            type: object
            properties:
              role:
                type: string
                enum: [user, assistant, system]
              content:
                type: string
            required: [role, content]
        temperature:
          type: number
          default: 0.7
          minimum: 0
          maximum: 1
          description: "Controls randomness in the response. Lower is more deterministic."
        max_tokens:
          type: integer
          default: 1024
          description: "Maximum number of tokens to generate."
        top_p:
          type: number
          default: 1
          minimum: 0
          maximum: 1
          description: "Controls diversity via nucleus sampling."
        stream:
          type: boolean
          default: true
          description: "Whether to stream the response."
        seed:
          type: integer
          description: "Seed for deterministic generation. Defaults to random if not provided."
        stop:
          oneOf:
            - type: string
            - type: array
              items:
                type: string
          description: "Sequence where the API will stop generating tokens."
        logprobs:
          type: boolean
          default: false
          description: "Whether to return log probabilities of the output tokens."
      required: [model, messages]

  - path: /completions
    externalPath: /completions
    method: POST
    auth:
      type: header
      key: "Authorization"
      value: "Bearer {{api-key}}"
    headers:
      Content-Type: application/json
    requestSchema:
      type: object
      properties:
        model:
          type: string
          default: "deepseek-ai/DeepSeek-V3-0324"
          enum:
            [
              "deepseek-ai/DeepSeek-V3-0324",
              "chutesai/Mistral-Small-3.1-24B-Instruct-2503",
              "deepseek-ai/DeepSeek-R1",
              "chutesai/Llama-4-Maverick-17B-128E-Instruct-FP8",
              "deepseek-ai/DeepSeek-V3",
              "Qwen/Qwen2.5-VL-32B-Instruct",
              "deepseek-ai/DeepSeek-R1-Zero",
              "nvidia/Llama-3_1-Nemotron-Ultra-253B-v1",
              "chutesai/Llama-4-Scout-17B-16E-Instruct",
              "deepseek-ai/DeepSeek-V3-Base",
              "moonshotai/Kimi-VL-A3B-Thinking",
              "cognitivecomputations/Dolphin3.0-Mistral-24B",
              "cognitivecomputations/Dolphin3.0-R1-Mistral-24B",
              "microsoft/MAI-DS-R1-FP8",
              "THUDM/GLM-4-32B-0414",
              "agentica-org/DeepCoder-14B-Preview",
              "shisa-ai/shisa-v2-llama3.3-70b",
              "THUDM/GLM-Z1-32B-0414",
              "ArliAI/QwQ-32B-ArliAI-RpR-v1",
              "chutesai/Llama-3.1-405B-FP8",
            ]
          description: "The model to use for completion."
        prompt:
          type: string
          description: "Text to generate completion for."
        temperature:
          type: number
          default: 0.7
          minimum: 0
          maximum: 1
          description: "Controls randomness in the response. Lower is more deterministic."
        max_tokens:
          type: integer
          default: 1024
          description: "Maximum number of tokens to generate."
        top_p:
          type: number
          default: 1
          minimum: 0
          maximum: 1
          description: "Controls diversity via nucleus sampling."
        stream:
          type: boolean
          default: true
          description: "Whether to stream the response."
        seed:
          type: integer
          description: "Seed for deterministic generation. Defaults to random if not provided."
        stop:
          oneOf:
            - type: string
            - type: array
              items:
                type: string
          description: "Sequence where the API will stop generating tokens."
        logprobs:
          type: boolean
          default: false
          description: "Whether to return log probabilities of the output tokens."
      required: [model, prompt]
