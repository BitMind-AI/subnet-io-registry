baseUrl: "https://llm.chutes.ai/v1"
endpoints:
  - path: /chat/completions
    externalPath: /chat/completions
    method: POST
    auth:
      type: header
      key: "Authorization"
      value: "Bearer {{api-key}}"
    headers:
      Content-Type: application/json
    requestSchema:
      type: object
      properties:
        model:
          type: string
          default: "deepseek-ai/DeepSeek-V3-0324"
          enum: [
            "deepseek-ai/DeepSeek-V3-0324",
            "chutesai/Mistral-Small-3.1-24B-Instruct-2503",
            "deepseek-ai/DeepSeek-R1",
            "chutesai/Llama-4-Maverick-17B-128E-Instruct-FP8",
            "deepseek-ai/DeepSeek-V3",
            "Qwen/Qwen2.5-VL-32B-Instruct",
            "deepseek-ai/DeepSeek-R1-Zero",
            "nvidia/Llama-3_1-Nemotron-Ultra-253B-v1",
            "chutesai/Llama-4-Scout-17B-16E-Instruct",
            "deepseek-ai/DeepSeek-V3-Base",
            "moonshotai/Kimi-VL-A3B-Thinking",
            "nvidia/Llama-3_3-Nemotron-Super-49B-v1",
            "unsloth/gemma-3-12b-it",
            "unsloth/gemma-3-1b-it",
            "unsloth/gemma-3-4b-it",
            "cognitivecomputations/Dolphin3.0-Mistral-24B",
            "nvidia/Llama-3.1-Nemotron-Nano-8B-v1",
            "NousResearch/DeepHermes-3-Llama-3-8B-Preview",
            "cognitivecomputations/Dolphin3.0-R1-Mistral-24B",
            "RekaAI/reka-flash-3",
            "open-r1/OlympicCoder-32B",
            "open-r1/OlympicCoder-7B"
          ]
          description: "The model to use for chat completion."
        messages:
          type: array
          items:
            type: object
            properties:
              role:
                type: string
                enum: [user, assistant, system]
              content:
                type: string
            required: [role, content]
        temperature:
          type: number
          default: 0.7
          minimum: 0
          maximum: 1
          description: "Controls randomness in the response. Lower is more deterministic."
        max_tokens:
          type: integer
          default: 1024
          description: "Maximum number of tokens to generate."
        top_p:
          type: number
          default: 1
          minimum: 0
          maximum: 1
          description: "Controls diversity via nucleus sampling."
        stream:
          type: boolean
          default: true
          description: "Whether to stream the response."
        seed:
          type: integer
          description: "Seed for deterministic generation. Defaults to random if not provided."
        stop:
          oneOf:
            - type: string
            - type: array
              items:
                type: string
          description: "Sequence where the API will stop generating tokens."
        logprobs:
          type: boolean
          default: false
          description: "Whether to return log probabilities of the output tokens."
      required: [model, messages]

  - path: /completions
    externalPath: /completions
    method: POST
    auth:
      type: header
      key: "Authorization"
      value: "Bearer {{api-key}}"
    headers:
      Content-Type: application/json
    requestSchema:
      type: object
      properties:
        model:
          type: string
          default: "deepseek-ai/DeepSeek-V3-0324"
          enum: [
            "deepseek-ai/DeepSeek-V3-0324",
            "chutesai/Mistral-Small-3.1-24B-Instruct-2503",
            "deepseek-ai/DeepSeek-R1",
            "chutesai/Llama-4-Maverick-17B-128E-Instruct-FP8",
            "deepseek-ai/DeepSeek-V3",
            "Qwen/Qwen2.5-VL-32B-Instruct",
            "deepseek-ai/DeepSeek-R1-Zero",
            "nvidia/Llama-3_1-Nemotron-Ultra-253B-v1",
            "chutesai/Llama-4-Scout-17B-16E-Instruct",
            "deepseek-ai/DeepSeek-V3-Base",
            "moonshotai/Kimi-VL-A3B-Thinking",
            "nvidia/Llama-3_3-Nemotron-Super-49B-v1",
            "unsloth/gemma-3-12b-it",
            "unsloth/gemma-3-1b-it",
            "unsloth/gemma-3-4b-it",
            "cognitivecomputations/Dolphin3.0-Mistral-24B",
            "nvidia/Llama-3.1-Nemotron-Nano-8B-v1",
            "NousResearch/DeepHermes-3-Llama-3-8B-Preview",
            "cognitivecomputations/Dolphin3.0-R1-Mistral-24B",
            "RekaAI/reka-flash-3",
            "open-r1/OlympicCoder-32B",
            "open-r1/OlympicCoder-7B"
          ]
          description: "The model to use for completion."
        prompt:
          type: string
          description: "Text to generate completion for."
        temperature:
          type: number
          default: 0.7
          minimum: 0
          maximum: 1
          description: "Controls randomness in the response. Lower is more deterministic."
        max_tokens:
          type: integer
          default: 1024
          description: "Maximum number of tokens to generate."
        top_p:
          type: number
          default: 1
          minimum: 0
          maximum: 1
          description: "Controls diversity via nucleus sampling."
        stream:
          type: boolean
          default: true
          description: "Whether to stream the response."
        seed:
          type: integer
          description: "Seed for deterministic generation. Defaults to random if not provided."
        stop:
          oneOf:
            - type: string
            - type: array
              items:
                type: string
          description: "Sequence where the API will stop generating tokens."
        logprobs:
          type: boolean
          default: false
          description: "Whether to return log probabilities of the output tokens."
      required: [model, prompt]