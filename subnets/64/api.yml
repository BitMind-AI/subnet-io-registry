baseUrl: "https://llm.chutes.ai/v1"
name: "Serverless Computing (Subnet 64)"
description: "Run high-performance models like DeepSeek on serverless infrastructure."
endpoints:
  - path: /chat/completions
    externalPath: /chat/completions
    method: POST
    summary: "Chat with high-performance AI models"
    description: "Engage in conversation using a wide range of high-performance language models (like DeepSeek, Qwen, Llama variants, etc.) available through Bittensor Subnet 64's serverless infrastructure. Suitable for complex Q&A, coding, and advanced text generation. See 'model' parameter for the extensive list of choices."
    auth:
      type: header
      key: "Authorization"
      value: "Bearer {{api-key}}"
    headers:
      Content-Type: application/json
    requestSchema:
      type: object
      properties:
        model:
          type: string
          description: "The model to use for chat completion."
          enum:
            [
              "deepseek-ai/DeepSeek-V3-0324",
              "chutesai/Mistral-Small-3.1-24B-Instruct-2503",
              "deepseek-ai/DeepSeek-R1",
              "chutesai/Llama-4-Maverick-17B-128E-Instruct-FP8",
              "deepseek-ai/DeepSeek-V3",
              "Qwen/Qwen2.5-VL-32B-Instruct",
              "deepseek-ai/DeepSeek-R1-Zero",
              "nvidia/Llama-3_1-Nemotron-Ultra-253B-v1",
              "chutesai/Llama-4-Scout-17B-16E-Instruct",
              "deepseek-ai/DeepSeek-V3-Base",
              "moonshotai/Kimi-VL-A3B-Thinking",
              "cognitivecomputations/Dolphin3.0-Mistral-24B",
              "cognitivecomputations/Dolphin3.0-R1-Mistral-24B",
              "microsoft/MAI-DS-R1-FP8",
              "THUDM/GLM-4-32B-0414",
              "agentica-org/DeepCoder-14B-Preview",
              "shisa-ai/shisa-v2-llama3.3-70b",
              "THUDM/GLM-Z1-32B-0414",
              "ArliAI/QwQ-32B-ArliAI-RpR-v1",
              "chutesai/Llama-3.1-405B-FP8",
            ]
          default: "chutesai/Mistral-Small-3.1-24B-Instruct-2503"
        messages:
          type: array
          items:
            type: object
            properties:
              role:
                type: string
                description: "Role of the message sender"
                enum: [user, assistant, system]
                default: "user"
              content:
                type: string
            required: [role, content]
        temperature:
          type: number
          default: 0.7
          minimum: 0
          maximum: 1
          description: "Controls randomness in the response. Lower is more deterministic."
        max_tokens:
          type: integer
          default: 1024
          description: "Maximum number of tokens to generate."
        top_p:
          type: number
          default: 1
          minimum: 0
          maximum: 1
          description: "Controls diversity via nucleus sampling."
        stream:
          type: boolean
          default: true
          description: "Whether to stream the response."
        seed:
          type: integer
          description: "Seed for deterministic generation. Defaults to random if not provided."
        stop:
          oneOf:
            - type: string
            - type: array
              items:
                type: string
          description: "Sequence where the API will stop generating tokens."
        logprobs:
          type: boolean
          default: false
          description: "Whether to return log probabilities of the output tokens."
      required: [model, messages]

  - path: /completions
    externalPath: /completions
    method: POST
    summary: "Generate text completions using high-performance models"
    description: "Completes a given text prompt using a wide range of high-performance language models (like DeepSeek, Qwen, Llama variants, etc.) available through Bittensor Subnet 64's serverless infrastructure. Ideal for demanding text generation tasks. See 'model' parameter for the extensive list of choices."
    auth:
      type: header
      key: "Authorization"
      value: "Bearer {{api-key}}"
    headers:
      Content-Type: application/json
    requestSchema:
      type: object
      properties:
        model:
          type: string
          description: "The model to use for completion."
          enum:
            [
              "deepseek-ai/DeepSeek-V3-0324",
              "chutesai/Mistral-Small-3.1-24B-Instruct-2503",
              "deepseek-ai/DeepSeek-R1",
              "chutesai/Llama-4-Maverick-17B-128E-Instruct-FP8",
              "deepseek-ai/DeepSeek-V3",
              "Qwen/Qwen2.5-VL-32B-Instruct",
              "deepseek-ai/DeepSeek-R1-Zero",
              "nvidia/Llama-3_1-Nemotron-Ultra-253B-v1",
              "chutesai/Llama-4-Scout-17B-16E-Instruct",
              "deepseek-ai/DeepSeek-V3-Base",
              "moonshotai/Kimi-VL-A3B-Thinking",
              "cognitivecomputations/Dolphin3.0-Mistral-24B",
              "cognitivecomputations/Dolphin3.0-R1-Mistral-24B",
              "microsoft/MAI-DS-R1-FP8",
              "THUDM/GLM-4-32B-0414",
              "agentica-org/DeepCoder-14B-Preview",
              "shisa-ai/shisa-v2-llama3.3-70b",
              "THUDM/GLM-Z1-32B-0414",
              "ArliAI/QwQ-32B-ArliAI-RpR-v1",
              "chutesai/Llama-3.1-405B-FP8",
            ]
          default: "chutesai/Mistral-Small-3.1-24B-Instruct-2503"
        prompt:
          type: string
          description: "Text to generate completion for."
        temperature:
          type: number
          default: 0.7
          minimum: 0
          maximum: 1
          description: "Controls randomness in the response. Lower is more deterministic."
        max_tokens:
          type: integer
          default: 1024
          description: "Maximum number of tokens to generate."
        top_p:
          type: number
          default: 1
          minimum: 0
          maximum: 1
          description: "Controls diversity via nucleus sampling."
        stream:
          type: boolean
          default: true
          description: "Whether to stream the response."
        seed:
          type: integer
          description: "Seed for deterministic generation. Defaults to random if not provided."
        stop:
          oneOf:
            - type: string
            - type: array
              items:
                type: string
          description: "Sequence where the API will stop generating tokens."
        logprobs:
          type: boolean
          default: false
          description: "Whether to return log probabilities of the output tokens."
      required: [model, prompt]
